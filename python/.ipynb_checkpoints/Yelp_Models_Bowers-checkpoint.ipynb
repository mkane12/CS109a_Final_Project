{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer as Tfidf\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as dt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn.decomposition import TruncatedSVD as SVD\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9126c5e18f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json_2015.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JohnBowers/anaconda/lib/python2.7/site-packages/pandas/io/json.pyc\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit)\u001b[0m\n\u001b[1;32m    209\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    210\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JohnBowers/anaconda/lib/python2.7/site-packages/pandas/io/json.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/JohnBowers/anaconda/lib/python2.7/site-packages/pandas/io/json.pyc\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 496\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('json_2015.json')\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#standardize predictor variables\n",
    "\n",
    "def scorer_pos(estimator, X, y): #custom scoring functions to get positive and negative accuracy\n",
    "    y_pred = estimator.predict(X)\n",
    "    return np.float(np.mean(y_pred[y == 1] == y[y == 1]))\n",
    "\n",
    "def scorer_neg(estimator, X, y):\n",
    "    estimator.fit(X, y)\n",
    "    y_pred = estimator.predict(X)\n",
    "    return np.float(np.mean(y_pred[y == 0] == y[y == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_model(x, binary_y, model, title, coefs=False):\n",
    "    model.fit(x, binary_y)\n",
    "    y_pred = model.predict(x)\n",
    "    binary_y = np.array(binary_y)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(binary_y, y_pred).ravel() #from sklearn docs\n",
    "\n",
    "    print title\n",
    "    print confusion_matrix(binary_y, y_pred)\n",
    "\n",
    "    print \"False Positive Rate:\", fp/float(fp+tn)\n",
    "    print \"False Negative Rate:\", fn/float(fn+tp)\n",
    "    print \"True Positive Rate:\", tp/float(tp+fp)\n",
    "    print \"True Negative Rate:\", tn/float(tn+fn)\n",
    "    \n",
    "    print \"Positive Accuracy:\", cross_val_score(model, x, binary_y, n_jobs=-1, scoring=scorer_pos).mean()\n",
    "    print \"Negative Accuracy:\", cross_val_score(model, x, binary_y, n_jobs=-1, scoring=scorer_neg).mean()\n",
    "    print \"Cross Validated Accuracy on Sample:\", cross_val_score(model, x, binary_y, n_jobs=-1).mean()\n",
    "    print \"Cross Validated AUC on Sample:\", cross_val_score(model, x, binary_y, n_jobs=-1, scoring='roc_auc').mean()\n",
    "    print \"Train Set Accuracy on Sample:\", np.mean(y_pred == binary_y)\n",
    "    print\n",
    "    if coefs == True:\n",
    "        mydict = zip(model.coef_[0], vectorizer.get_feature_names())\n",
    "        words = sorted([(i[0], i[1].encode('utf-8')) for i in mydict], reverse=True, key=lambda x: x[0])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-85b6150c1769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#draw samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#lengths = [len(i[1]['text']) for i in samp.iterrows()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "samp = data.sample(100000) #draw samples\n",
    "\n",
    "#lengths = [len(i[1]['text']) for i in samp.iterrows()]\n",
    "\n",
    "samp.shape\n",
    "frac_useful = 1 - data['useful'].value_counts()[0] / float(data.shape[0])\n",
    "frac_funny = 1 - data['funny'].value_counts()[0] / float(data.shape[0])\n",
    "frac_cool = 1 - data['cool'].value_counts()[0] / float(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             binary=False, \n",
    "                             max_features=10000,\n",
    "                             analyzer='word',\n",
    "                             #tokenizer=tokenize,\n",
    "                             sublinear_tf=False\n",
    "                            ) #using non-binary Count Vec.\n",
    "reviews = samp.text.values\n",
    "\n",
    "#tokenize words\n",
    "x = vectorizer.fit_transform(reviews)\n",
    "\n",
    "y_useful = [] #class observations according to whether they have at least one \"helpful\" vote\n",
    "y_cool = []\n",
    "y_funny = []\n",
    "\n",
    "\n",
    "for score in samp.useful.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    y_useful.append(i)\n",
    "\n",
    "for score in samp.cool.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    y_cool.append(i)\n",
    "\n",
    "for score in samp.funny.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    y_funny.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frac_useful' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4cc66e95b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc_tune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfrac_useful\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mu_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac_useful\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mu_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlog_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#initialize logistic regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frac_useful' is not defined"
     ]
    }
   ],
   "source": [
    "u_tune = 0.00\n",
    "f_tune = 0.00\n",
    "c_tune = 0.00\n",
    "\n",
    "weights = {0:1/(1 - frac_useful + u_tune), 1:1/(frac_useful - u_tune)}\n",
    "log_model = LogReg(C=0.01, penalty='l2', class_weight=weights) #initialize logistic regression model\n",
    "\n",
    "rand = RandomForestClassifier(n_estimators=300, criterion='gini', \n",
    "                              max_features= 17, max_depth=5, class_weight='balanced')\n",
    "\n",
    "\n",
    "\n",
    "useful_words = test_model(x, y_useful, \n",
    "                          log_model,\n",
    "                          \"Useful\",\n",
    "                          coefs=True)\n",
    "funny_words = test_model(x, y_funny, \n",
    "                         log_model.set_params(class_weight={0:1/(1 - frac_funny + f_tune), 1:1/(frac_funny - f_tune)}), \n",
    "                         \"Funny\",\n",
    "                          coefs=True)\n",
    "cool_words = test_model(x, y_cool, \n",
    "                        log_model.set_params(class_weight={0:1/(1 - frac_cool + c_tune), 1:1/(frac_cool - c_tune)}),\n",
    "                        #rand,\n",
    "                        \"Cool\",\n",
    "                          coefs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Coolest Words:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cool_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-789df6e8ae43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"10 Coolest Words:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcool_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n10 Least Cool Words:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cool_words' is not defined"
     ]
    }
   ],
   "source": [
    "print \"10 Coolest Words:\"\n",
    "for word in cool_words[:10]:\n",
    "    print word[1], word[0]\n",
    "\n",
    "print \"\\n10 Least Cool Words:\"\n",
    "for word in cool_words[-10:]:\n",
    "    print word[1], word[0]\n",
    "    \n",
    "print \"\\n10 Funniest Words:\"\n",
    "for word in funny_words[:10]:\n",
    "    print word[1], word[0]\n",
    "\n",
    "print \"\\n10 Least Funny Words:\"\n",
    "for word in funny_words[-10:]:\n",
    "    print word[1], word[0]\n",
    "    \n",
    "print \"\\n10 Most Useful Words:\"\n",
    "for word in useful_words[:10]:\n",
    "    print word[1], word[0]\n",
    "\n",
    "print \"\\n10 Least Useful Words:\"\n",
    "for word in useful_words[-10:]:\n",
    "    print word[1], word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful\n",
      "[[166388  82731]\n",
      " [ 58412  92469]]\n",
      "False Positive Rate: 0.332094300314\n",
      "False Negative Rate: 0.387139533805\n",
      "True Positive Rate: 0.52779109589\n",
      "True Negative Rate: 0.740160142349\n",
      "Positive Accuracy: 0.599452569747\n",
      "Negative Accuracy: 0.661314481825\n",
      "Cross Validated Accuracy on Sample: 0.634405008157\n",
      "Cross Validated AUC on Sample: 0.673775543874\n",
      "Train Set Accuracy on Sample: 0.6471425\n",
      "\n",
      "Funny\n",
      "[[224376 106569]\n",
      " [ 23641  45414]]\n",
      "False Positive Rate: 0.322014231972\n",
      "False Negative Rate: 0.342350300485\n",
      "True Positive Rate: 0.298809735299\n",
      "True Negative Rate: 0.904679921134\n",
      "Positive Accuracy: 0.628861110506\n",
      "Negative Accuracy: 0.672997023675\n",
      "Cross Validated Accuracy on Sample: 0.662530004008\n",
      "Cross Validated AUC on Sample: 0.70180118236\n",
      "Train Set Accuracy on Sample: 0.674475\n",
      "\n",
      "Cool\n",
      "[[214325  98403]\n",
      " [ 34632  52640]]\n",
      "False Positive Rate: 0.314660024046\n",
      "False Negative Rate: 0.396828306903\n",
      "True Positive Rate: 0.348510026946\n",
      "True Negative Rate: 0.860891639922\n",
      "Positive Accuracy: 0.577413101408\n",
      "Negative Accuracy: 0.682833003663\n",
      "Cross Validated Accuracy on Sample: 0.654847498833\n",
      "Cross Validated AUC on Sample: 0.674034342037\n",
      "Train Set Accuracy on Sample: 0.6674125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "u_tune = 0.0\n",
    "f_tune = 0.0\n",
    "c_tune = 0.0\n",
    "\n",
    "weights = {0:1/(1 - frac_useful + u_tune), 1:1/(frac_useful - u_tune)}\n",
    "svm = LinearSVC(C=0.001, penalty='l2', class_weight='balanced') #initialize logistic regression model\n",
    "\n",
    "rand = RandomForestClassifier(n_estimators=500, criterion='gini', \n",
    "                              max_features= 17, max_depth=15, class_weight='balanced_subsample')\n",
    "\n",
    "\n",
    "useful_words = test_model(x, y_useful, svm, \"Useful\")\n",
    "funny_words = test_model(x, y_funny, \n",
    "                         svm, \"Funny\")\n",
    "cool_words = test_model(x, y_cool, \n",
    "                        svm, \"Cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
